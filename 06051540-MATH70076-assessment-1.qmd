---
title: "06051540-MATH70076-assessment-1"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "Justin Upson"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
bibliography: ref.bib 
---

# Question 1

**For $\xi \neq 0$:** 

Assuming $\sigma > 0,$ and $\xi$ in the real numbers. We know from the cumulative distribution function that:
\begin{equation}
  F(x;\sigma,\xi,u) = 1-\left(1+\frac{\xi(x-u)}{\sigma}\right)_+^{-1/\xi}
\end{equation}
For now, we can also drop the max function part (as it pertains to the range) and we can consider that separately later.
Setting $p = F(x;\sigma,\xi,u) \in [0,1]$, and rewriting to make $x$ the subject:
\begin{align}
\left(1+\frac{\xi(x-u)}{\sigma}\right)^{-1/\xi}&=(1-p) \\
1&=(1-p)\left(1+\frac{\xi(x-u)}{\sigma}\right)^{1/\xi} \\
(1-p)^{-1}&=\left(1+\frac{\xi(x-u)}{\sigma}\right)^{1/\xi} \\
\end{align}

For $\xi > 0$, we see that $\left(1+\frac{\xi(x-u)}{\sigma}\right)^{1/\xi}>0$, so we get $u \leq x$ with slowly decaying tails and:
\begin{align} 
(1-p)^{-\xi}&=1+\frac{\xi(x-u)}{\sigma} \\
(1-p)^{-\xi}-1&=\frac{\xi(x-u)}{\sigma} \\
\left(\left(1-p\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi} &= x-u\\
u+\left(\left(1-p\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi} &= x\\
\end{align}
So $F_X^{-1}(p;\sigma,\xi,u)=u+\left(\left(1-p\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi}=x$

For $\xi < 0$, we know that we have quickly decaying tails with finite upper endpoint. With $x>u$, this finite endpoint is met when
\begin{align}
1+\frac{\xi(x-u)}{\sigma}&=0 \\
\frac{\xi(x-u)}{\sigma}&=-1 \\
x-u&=-\frac{\sigma}{\xi} \\
x&=u-\frac{\sigma}{\xi} \\
\end{align}
So, in the event of $\xi < 0$, the range of our function is such that $u \leq x \leq \frac{u- \sigma}{\xi}$, for the function $F_X^{-1}(p;\sigma,\xi,u)=u+\left(\left(1-p\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi}=x$

It is given in the question that for $\xi \rightarrow 0$, the GPD reduces to an exponential distribution, hence continuous.

**For $\xi = 0$:**
\begin{align}
p&=1-exp\left(-\frac{x-u}{\sigma}\right) \\
exp\left(-\frac{x-u}{\sigma}\right)&=1-p \\
-\frac{x-u}{\sigma}&=ln(1-p) \\
-x+u&=\sigma ln(1-p) \\
u-\sigma ln(1-p) &= x \\
\end{align}
So for $\xi = 0$, we get that $F_X^{-1}(p;\sigma,u)=u-\sigma ln(1-p)=x$ and the range of our function is $u \leq x$


# Question 2a

Defining the quantile function in the style of qnorm() and qunif(), we need out code which produces the inverse CDF whilst providing similar error messages for inadmissable inputs. From this, we get that:
```{r}
qgpd <- function (p, sigma=1, xi=0, u=0){
  len <- max(length(p), length(sigma), length(xi), length(u))
  p <- rep_len(p, len)
  sigma <- rep_len(sigma, len)
  xi <- rep_len(xi, len)
  u <- rep_len(u, len)
  if (!is.numeric(p) || !is.numeric(sigma) || !is.numeric(xi) || !is.numeric(u)){
    return(warning("Errpr: object not found"))
  }
  else if (any(p<0|p>1, na.rm = TRUE)){
    return(warning("NaNs produced"))
  }
  else if (any(sigma <= 0, na.rm = TRUE)){
    return(warning("NaNs produced"))
  }
  
  safe_xi <- ifelse(xi==0, 1, xi)
  
  value <- ifelse(
    xi == 0,
    u - sigma * log1p(-p),
    u + ((1 - p)^(-xi) - 1) * sigma / safe_xi)
  maxp <- u - sigma/safe_xi
  
  
  
  # Ensuring our range is correct
  value <- ifelse(xi<0 & p==1, maxp, value)
  value <- ifelse(xi<0 & p<1 & maxp <= value, NaN, value)
  
  inadmissable <-
  (is.na(p))|(is.na(sigma))|(is.na(xi))|(is.na(u))|(!is.numeric(p))|(!is.numeric(sigma))|(!is.numeric(xi))|(!is.numeric(u))| (p < 0)|(p > 1)|(sigma <= 0)

  if (any(inadmissable, na.rm = TRUE)) warning("NaNs produced")
  value[inadmissable] <- NaN
  
  value
  
}

```

By default, the expected inputs for the function are sigma=1, xi=0, u=0. The code also prevents inputs where p values are less than 0, where p values exceed 1, or where sigma is less than or equal to 0.

The expected output is a real number greater than u that is unbounded if xi is greater than or equal to 0. The expected output is less than u - sigma/xi if xi is less than 0.

Regarding the behaviours of the quantile function:
The larger the value of xi, the slower the tail decays. In this for xi $>=$ 0 the functions output approaches infinity as p -> 1.
For xi < 0, the functions output has a maximum at u - sigma/xi (when p -> 1).
For larger values of sigma, the slower the tail decays.


# Question 2b

```{r}
qgpd(0.5,2,-0.4,1.5)
qgpd(0.75,2,-0.4,1.5)
qgpd(0.99,2,-0.4,1.5)
```


# Question 3

Graphing our actual vs expected split by id, we see that: 
```{r}
#| echo: false

# Reading our inputs:
gpd_parameters <- read.csv("gpd_parameters.csv")
gpd_samples <- read.csv("gpd_samples.csv")

# Mapping from our samples vector to our parameters table: 
row <- match(gpd_samples$set_id, gpd_parameters$id)

# Ranking our samples:
class_ranking <- ave(gpd_samples$value, gpd_samples$set_id, FUN = rank)

# Using our samples to generate our p-values:
n <- gpd_parameters$size[row]
p <- (class_ranking - 0.5) / n


# Formatting (necessary for the legend):
f   <- factor(gpd_samples$set_id)
cols <- as.integer(f)
ptch <- 16
pt.cex <- 0.6

expected_q <- qgpd(
  p, 
  gpd_parameters$sigma[row], 
  gpd_parameters$xi[row],
  gpd_parameters$u[row]
)

# Graphing our actual vs expected (using our p-values to generate our expected):
plot(
x = gpd_samples$value, 
y = expected_q,
xlab = "Actuals",
ylab = "Expected",
col = cols,
pch = ptch,
cex = pt.cex
)

# Adding our y=x line:
abline(0,1,col="red")

# Adding our legend:
legend(
  "bottomright",
  title = "id",
  legend = levels(f),
  col = 1:6,
  pch = ptch,
  cex = pt.cex,
  bty = "o")

```
From the figure above, we see some interesting results -- although most actuals seem in line with the expecteds (being close to the figures red line), this is not always the case. Ultimately, however, the different ids need to be separated so as to provide a more granular analysis of the distributions:

```{r}
#| echo: false

# Making six QQ plots (with one per id):
ids <- levels(f)


# Setting up the grid for our 6 smaller QQ plots:
op <- par(mfrow = c(2, 3))

# Building our plots:
for(id in ids){
  filt <- gpd_samples$set_id == id
  limits <- range(c(gpd_samples$value[filt], expected_q[filt]), finite = TRUE)
  plot(
    x = gpd_samples$value[filt],
    y = expected_q[filt],
    xlab = "Actuals",
    ylab = "Expected",
    main = paste("id:", id),
    pch  = ptch,
    cex  = pt.cex,
    xlim = limits,
    ylim = limits,
    asp  = 1
  )
  abline(0, 1, col = "red")
}

# Reverting from grids back to normal:
par(op)

cor_results <- tapply(
  expected_q,
  gpd_samples$set_id,
  function(expected_subset) {
    filt <- gpd_samples$set_id == names(expected_subset)[1]
    cor(gpd_samples$value[filt], expected_q[filt])
  }
)

# Displaying correlations by id:
cor_results

```
From the above, we see that:
- Values taken from a are slightly beneath the $y=x$ line for smaller values and significantly beneath it for larger values
- Values taken from b are consistently on the $y=x$ line
- Values taken from c are beneath the $y=x$ line
- Values taken from d are consistently above the $y=x$ line
- Values taken from e are slightly above the $y=x$ line
- Values taken from f are on the $y=x$ line for smaller values but are significantly beneath the line for larger values

Given that results which deviate from the $y=x$ line show poor fit between the actual and the expected data, it is reasonable that 









# Question 4

```{r}
#| echo: false

# Reading our inputs:
riverflow <- read.csv("riverflow_2015_2024.csv")

# Formating dates:
riverflow$date <- as.Date(riverflow$date, format = "%Y-%m-%d")

# Time Series of flow data:
plot(
x = riverflow$date,
y = riverflow$flow,
type = "l",
col = "blue",
xlim = range(riverflow$date, na.rm = TRUE),
ylim = range(riverflow$flow, na.rm = TRUE),
xlab = "Year",
ylab = "River Flow (cumecs)",
main = "River Flow Time Series (2015–2024)"
)

# Time Series of baseline mean:
lines(
x = riverflow$date,
y = riverflow$baseline_mean,
col = "red",
lty = c(1, 2),
lwd = c(1, 2))

# Legend:
legend(
"topright",
legend = c("Flow", "Baseline mean"),
col = c("blue", "red"), 
bty = "o")


# Time series plot of flow minus baseline mean. 
plot(
x = riverflow$date,
y = riverflow$flow - riverflow$baseline_mean,
type = "l",
xlab = "Date", 
ylab = "Flow − baseline (cumecs)",
main = "Deviation from baseline"
)
abline(h = 0, lty = 3)


```

Determining whether the distribution of river flows is constant over time, we can see from the above that an assumption of constant flow is flawed. This is because we see that the speed of the river's flow varies tremendously - at numerous (albeit brief) intervals over the ten year period, the speed of the river is many multiples of the mean river speed. This is something we can demonstrably see when comparing the mean red dashed line with the time series plot above.


```{r echo=FALSE}

# Adding another column:
riverflow$month <- factor(format(riverflow$date, "%b"), levels = month.abb)

# Box and Whisker plot:
boxplot(
flow ~ month,
data = riverflow,
ylab = "River Flow (cumecs)",
xlab = "Month",
main = "Monthly distribution of flow",
notch = TRUE,
outline = FALSE)
abline(h = mean(riverflow$flow), col = "red", lwd = 2, lty = 2)
legend("topright", legend = "Overall mean", col = "red", lwd = 2, lty = 2, bty = "n")
```

By using a box and whisker plot which considers each month, we can see how seasonal changes cause profound effects in the speed of the river's flow. The month of May, for example has an interquartile range in river flow speeds that exceed the entire interquartile range in river flow speeds from the month of March.



```{r}
#| echo: false

# Set up, so that we can use the formula from question 2:
over75 <- riverflow$flow[riverflow$flow > 75]
p <- rank(over75-0.5)/length(over75)

# Quantile Quantile Plot comparing the actual versus expected data:
plot(
x = sort(over75)-75,
y = sort(qgpd(p,29.7,0.62,0)), 
xlab = "Actual",
ylab = "Expected",
main = "Generalised Pareto Distribution Quantile Quantile plot for exceedances over 75")
abline(0, 1, col = "red")

```

To determine whether it is appropriate to model large data flows with the stated GPD, I constructed a quantile quantile plot contrasting the actual dataset, and the expected data. In this, the expected data was calculated using the inverse cumulative distribution.

From the graph, we can see that it is not appropriate to model large data flows with the GPD provided in the question. This is because we see a significant deviation of the plotted points from the $y=x$ line - the expected values don't align with the actual values when dealing with the largest river flows.


# Code appendix

```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```

# References



---

