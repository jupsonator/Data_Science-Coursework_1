---
title: "06051540-MATH70076-assessment-1"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "Justin Upson"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
bibliography: ref.bib 
---

# Question 1

**For $\xi \neq 0$:** 

We know from the cumulative distribution function that:
\begin{equation}
  F(x;\sigma,\xi,u) = 1-\left(1+\frac{\xi(x-u)}{\sigma}\right)_+^{-1/\xi}
\end{equation}
Rewriting to make $x$ the subject:
\begin{align}
\left(1+\frac{\xi(x-u)}{\sigma}\right)_+^{-1/\xi}&=(1-F) \\
1&=(1-F)\left(1+\frac{\xi(x-u)}{\sigma}\right)_+^{1/\xi} \\
(1-F)^{-1}&=\left(1+\frac{\xi(x-u)}{\sigma}\right)_+^{1/\xi} \\
\end{align}
For $\xi > 0$, we see that $\left(1+\frac{\xi(x-u)}{\sigma}\right)^{1/\xi}>0$, so we get:
\begin{align}
(1-F)^{-\xi}&=1+\frac{\xi(x-u)}{\sigma} \\
(1-F)^{-\xi}-1&=\frac{\xi(x-u)}{\sigma} \\
\left(\left(1-F\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi} &= x-u\\
u+\left(\left(1-F\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi} &= x\\
\end{align}
So $F_X^{-1}=u+\left(\left(1-x\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi}$
But given our inputs of $F_X^{-1}(x)$ vary between 0 and 1, we can write that:
\begin{equation}
F_X^{-1}(x)=u+\left(\left(x\right)^{-\xi}-1\right)\times \frac{\sigma}{\xi}
\end{equation}

For $\xi < 0$, we know that we have quickly decaying tails with finite upper endpoint. With $x>u$, this finite endpoint is met when
\begin{align}
1+\frac{\xi(x-u)}{\sigma}&=0 \\
\frac{\xi(x-u)}{\sigma}&=-1 \\
x-u&=-\frac{\sigma}{\xi} \\
x&=u-\frac{\sigma}{\xi} \\
\end{align}
So for $x > u-\frac{\sigma}{\xi}$, produced by the inverse function above we discard the values of $x$.

**For $\xi = 0$:**
\begin{align}
F&=1-exp\left(-\frac{x-u}{\sigma}\right) \\
exp\left(-\frac{x-u}{\sigma}\right)&=1-F \\
-\frac{x-u}{\sigma}&=ln(1-F) \\
-x+u&=\sigma ln(1-F) \\
u-\sigma ln(1-F) &= x \\
\end{align}
So $F_X^{-1}=u-\sigma ln(1-x)$

But given our inputs of $F_X^{-1}(x)$ vary between 0 and 1, we can write that:
\begin{equation}
F_X^{-1}(x)=u-\sigma ln(x)
\end{equation}

# Question 2a

Defining the quantile function, and using the basis of the cdf from question 1, we get that:
```{r}
qgpd <- function (p, sigma=1, xi=0, u=0){
  if (p<0||p>1){
    return(warning("NaNs produced - p must be between 0 and 1")) 
  }
  else if (sigma<=0){
    return(warning("NaNs produced - sigma must be greater than 0")) 
  }
  else if (xi != 0){
    return(u + ((1-p)^(-xi)-1) * sigma / xi)
  } else {
    return(u - sigma * log(1-p))
  }
}
```

By default, the expected inputs for the function are sigma=1, xi=0, u=0. The code also prevents inputs where p values are less than 0, where p values exceed 1, or where sigma is less than or equal to 0.

The expected output is a real number greater than u that is unbounded if xi is greater than or equal to 0. The expected output is less than u - sigma/xi if xi is less than 0.

Regarding the behaviours of the quantile function:
The larger the value of xi, the slower the tail decays. In this for xi $>=$ 0 the functions output approaches infinity as p -> 1.
For xi < 0, the functions output has a maximum at u - sigma/xi (when p -> 1).
For larger values of sigma, the slower the tail decays.


# Question 2b

```{r}
qgpd(0.5,2,-0.4,1.5)
qgpd(0.75,2,-0.4,1.5)
qgpd(0.99,2,-0.4,1.5)
```


# Question 3

Graphing our actual vs expected split by id, we see that: 
```{r}
#| echo: false

# Reading our inputs:
gpd_parameters <- read.csv("gpd_parameters.csv")
gpd_samples <- read.csv("gpd_samples.csv")

# Mapping from our samples vector to our parameters table: 
row <- match(gpd_samples$set_id, gpd_parameters$id)

# Ranking our samples:
class_ranking <- ave(gpd_samples$value, gpd_samples$set_id, FUN = rank)

# Using our samples to generate our p-values:
n <- gpd_parameters$size[row]
p <- (class_ranking - 0.5) / n

# To enable vectors as inputs:
qgpd_vec <- function(p, sigma=1, xi=0, u=0){
  ifelse(
    rep_len(xi, length(p)) == 0,
    u - sigma * log(1 - p), 
    u + ((1 - p)^(-xi) - 1) * sigma / xi 
  )
}

# Formatting (necessary for the legend):
f   <- factor(gpd_samples$set_id)
cols <- as.integer(f)
ptch <- 16
pt.cex <- 0.6

expected_q <- qgpd_vec(
  p, 
  gpd_parameters$sigma[row], 
  gpd_parameters$xi[row],
  gpd_parameters$u[row]
)

# Graphing our actual vs expected (using our p-values to generate our expected):
plot(
x = gpd_samples$value, 
y = expected_q,
xlab = "Actuals",
ylab = "Expected",
col = cols,
pch = ptch,
cex = pt.cex
)

# Adding our y=x line:
abline(0,1,col="red")

# Adding our legend:
legend(
  "bottomright",
  title = "id",
  legend = levels(f),
  col = 1:6,
  pch = ptch,
  cex = pt.cex,
  bty = "o")

```
From the figure above, we see some interesting results -- although most actuals seem in line with the expecteds (being close to the figures red line), this is not always the case. Ultimately, however, the different ids need to be separated so as to provide a more granular analysis of the distributions:

```{r}
#| echo: false

# Making six QQ plots (with one per id):
ids <- levels(f)
limits <- range(c(gpd_samples$value, expected_q), finite = TRUE)

# Setting up the grid for our 6 smaller QQ plots:
op <- par(mfrow = c(2, 3))

# Building our plots:
for(id in ids){
  ii <- gpd_samples$set_id == id
  plot(
    x = gpd_samples$value[ii],
    y = expected_q[ii],
    xlab = "Actuals",
    ylab = "Expected",
    main = paste("id:", id),
    pch  = ptch,
    cex  = pt.cex,
    xlim = limits,
    ylim = limits,
    asp  = 1
  )
  abline(0, 1, col = "red")
}

# Reverting from grids back to normal:
par(op)

```
From these smaller figures, we can much more readily see which of the actual distributions align with the expected distributions. We can therefore see that:





# Question 4

```{r}
#| echo: false

# Reading our inputs:
riverflow <- read.csv("riverflow_2015_2024.csv")

# Formating dates:
riverflow$date <- as.Date(riverflow$date, format = "%Y-%m-%d")

# Time Series of flow data:
plot(
x = riverflow$date,
y = riverflow$flow,
type = "l",
col = "blue",
xlim = range(riverflow$date, na.rm = TRUE),
ylim = range(riverflow$flow, na.rm = TRUE),
xlab = "Year",
ylab = "River Flow (cumecs)",
main = "River Flow Time Series (2015–2024)"
)

# Time Series of baseline mean:
lines(
x = riverflow$date,
y = riverflow$baseline_mean,
col = "red",
lty = c(1, 2),
lwd = c(1, 2))

# Legend:
legend(
"topright",
legend = c("Flow", "Baseline mean"),
col = c("blue", "red"), 
bty = "o")


# Time series plot of flow minus baseline mean. 
plot(
x = riverflow$date,
y = riverflow$flow - riverflow$baseline_mean,
type = "l",
xlab = "Date", 
ylab = "Flow − baseline (cumecs)",
main = "Deviation from baseline"
)
abline(h = 0, lty = 3)


```

Determining whether the distribution of river flows is constant over time, we can see from the above that an assumption of constant flow is flawed. This is because we see that the speed of the river's flow varies tremendously - at numerous (albeit brief) intervals over the ten year period, the speed of the river is many multiples of the mean river speed. This is something we can demonstrably see when comparing the mean red dashed line with the time series plot above.


```{r echo=FALSE}

# Adding another column:
riverflow$month <- factor(format(riverflow$date, "%b"), levels = month.abb)

# Box and Whisker plot:
boxplot(
flow ~ month,
data = riverflow,
ylab = "River Flow (cumecs)",
xlab = "Month",
main = "Monthly distribution of flow",
notch = TRUE,
outline = FALSE)
abline(h = mean(riverflow$flow), col = "red", lwd = 2, lty = 2)
legend("topright", legend = "Overall mean", col = "red", lwd = 2, lty = 2, bty = "n")
```

By using a box and whisker plot which considers each month, we can see how seasonal changes cause profound effects in the speed of the river's flow. The month of May, for example has an interquartile range in river flow speeds that exceed the entire interquartile range in river flow speeds from the month of March.



```{r}
#| echo: false

# Set up, so that we can use the formula from question 2:
over75 <- riverflow$flow[riverflow$flow > 75]
p <- rank(over75-0.5)/length(over75)

# Quantile Quantile Plot comparing the actual versus expected data:
plot(
x = sort(over75)-75,
y = sort(qgpd_vec(p,29.7,0.62,0)), 
xlab = "Actual",
ylab = "Expected",
main = "Generalised Pareto Distribution Quantile Quantile plot for exceedances over 75")
abline(0, 1, col = "red")

```

To determine whether it is appropriate to model large data flows with the stated GPD, I constructed a quantile quantile plot contrasting the actual dataset, and the expected data. In this, the expected data was calculated using the inverse cumulative distribution.

From the graph, we can see that it is not appropriate to model large data flows with the GPD provided in the question. This is because we see a significant deviation of the plotted points from the $y=x$ line - the expected values don't align with the actual values when dealing with the largest river flows.

---

